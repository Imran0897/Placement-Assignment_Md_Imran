{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1W5ErFnqevHU1ispJyXeYuOz9chmv4kV5",
      "authorship_tag": "ABX9TyNkJ752MLXuE7R0fVo+PZ4z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Imran0897/Placement-Assignment_Md_Imran/blob/main/NLP_03_Placement_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8uW_dLN1YqG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "import spacy\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the extracted text from the CSV file\n",
        "csv_file = '/content/drive/MyDrive/Placement_Task/NLP_03/extracted_text.csv'\n",
        "df = pd.read_csv(csv_file,)\n",
        "text = df['Text'].values[0]"
      ],
      "metadata": {
        "id": "fUGyINof5xY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the text using spaCy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "xKkD2hKh52dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract keywords using spaCy's noun phrases\n",
        "keywords = []\n",
        "for chunk in doc.noun_chunks:\n",
        "    if chunk.text and not chunk.root.is_stop:\n",
        "        keywords.append(chunk.text)"
      ],
      "metadata": {
        "id": "QPWK1NwUBrqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform topic modeling using LDA\n",
        "tokenized_text = gensim.utils.simple_preprocess(text, deacc=True, min_len=2)\n",
        "dictionary = corpora.Dictionary([tokenized_text])\n",
        "corpus = [dictionary.doc2bow(tokenized_text)]\n",
        "lda_model = gensim.models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=10)"
      ],
      "metadata": {
        "id": "YQrlg7ocB-eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the most dominant topic based on topic probabilities\n",
        "dominant_topic = max(lda_model[corpus], key=lambda x: max(x, key=lambda item: item[1]))[0]\n",
        "# Print the keywords and the most dominant topic\n",
        "#topic_keywords = lda_model.show_topic(dominant_topic)\n",
        "\n",
        "print(\"Keywords:\", keywords)\n",
        "#print(\"Most Dominant Topic:\", topic_keywords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGdvQGYvCBLA",
        "outputId": "d403f497-bcd3-468f-d2dd-68aa4d30fe64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keywords: ['1 0 0\\nE', 'e', 'o', 'a r k s\\nQuestion1:-\\nWriteaprogramthattakesastringasinput', 'andcountsthefrequencyofeachwordinthestring', 'Yourtaskistofindthehighestfrequencyandreturnsthelengthofthehighest-frequencyword', 'Note-Youhavetowriteatleast2additionaltestcasesinwhichyourprogramwillrunsuccessfullyandprovideanexplanationforthesame', 'Exampleinput-string=‚Äúwritewritewriteallthenumberfromfromfrom1to100‚Äù\\nExampleoutput-5\\nExplanation-Fromthegivenstringwecannotethatthemostfrequentwordsare‚Äúwrite‚Äùand‚Äúfrom‚Äùandthemaximumvalueofboththevaluesis‚Äúwrite‚Äùanditscorrespondinglengthis5', 'Itisalsovalidifhecanremovejustonecharacterattheindexinthestring', 'Ifso', 'return\\nYES', 'Note-Youhavetowriteatleast2additionaltestcasesinwhichyourprogramwillrunsuccessfullyandprovideanexplanationforthesame', 'Exampleinput1', 's=‚Äúabc‚Äù', 'Thisstringisnotvalidaswecanremoveonly1occurrenceof‚Äúc‚Äù', 'whichwoulddownloadthedatafromtheprovidedlink', 'andthenreadthedataandconvertthatintoproperlystructureddataandreturnitinExcelformat', 'IdentificationNumber-intnum', 'Numberofthe\\n‚óèPok√©monintheofficialPok√©dex-intname', 'Pok√©monname-', 'URLtoanimageofthisPok√©mon-stringtype', '‚óèPok√©montype-stringheight', 'Pok√©monheight-float\\n‚óè', 'weight', 'Pok√©monweight-floatcandy', 'typeofcandyusedtoevolvePok√©monor', ':theamountofcandiesrequiredtoevolve\\n-int', 'egg', 'floatspawn_chance', '‚óè', 'Numberofthis\\npokemonon10.000spawns(NEW)-int\\n‚óè', 'spawn_time', 'Spawnsmostactiveatthetimeonthisfield', 'Spawntimesarethesameforall\\ntimezonesandareexpressedinlocaltime.(NEW)-‚Äúminutes', 'seconds‚Äùmultipliers', 'MultiplierofCombatPower(CP)forcalculatingtheCPafterevolutionSeebelow-listofint\\nweakness', 'Typesof\\n‚óèPok√©monthisPok√©monisweakto-listofstringsnext_evolution', 'successiveevolutionsofPok√©mon-listofdictprev_evolution', 'NumberandNameofprevious\\nevolutionsofPok√©mon', 'Link-https://data.nasa.gov/resource/y77d-th95.json\\nNote-Writecodecommentswhereverneededforcodeunderstanding', 'SampleData-', 'NameofEarthMeteorite-stringid-IDofEarth\\n‚óè', 'Meteorite-intnametype-stringrecclass-string', 'MassofEarthMeteorite', 'Meteoritewashit-datetimeformatreclat-floatrecclong-float', 'pointcoordinates-listofint\\nQuestion5-', 'id-inturl-string', '-intnumber', 'type-stringairdate-', '‚óè12-hourtimeformat \\n‚óèruntime-float \\n‚óèaveragerating-float', 'summary-string', 'mediumimagelink-string', '2.Writecodecommentswhereverrequiredforcodeunderstanding\\nInsightstobedrawn-\\n‚óèGetallPokemonswhosespawnrateislessthan5% \\n‚óè', 'Note', 'second‚Äùformatandperformtheanalysis', '‚óèGetallPokemonwhohavemorethantwotypesofcapabilities\\nQuestion7-', '2.Writecodecommentswhereverrequiredforcodeunderstanding\\nInsightstobedrawn-', 'GetalltheEarthmeteoritesthatfellbeforetheyear2000', 'Getalltheearthmeteoritesco-ordinateswhofellbeforetheyear1970', 'Assumingthatthemassoftheearthmeteoriteswasinkg', 'getallthosewhosemasswasmorethan10000kgQuestion8-', 'UsingthedatafromQuestion5,writecodetheanalyzethedataandanswerthefollowingquestionsNote-\\n1.Drawplotstodemonstratetheanalysisforthefollowingquestionsandbettervisualizations', '2.Writecodecommentswhereverrequiredforcodeunderstanding\\nInsightstobedrawn-', '‚óè', 'likeseason1ratings', 'season2,andsoon', '‚óèGetalltheepisodenames', 'whoseaverageratingismorethan8foreveryseason', '‚óè', 'GetalltheepisodenamesthatairedbeforeMay2019 \\n‚óè', 'Getthesummaryforthemostpopular(ratings)episodeineveryseason', 'Question9-', 'performdataanalysisandanswerthefollowingquestions', 'Getallthecarsandtheirtypesthatdonotqualifyforcleanalternativefuelvehicle \\n‚óèGetallTESLAcarswiththemodelyear', 'andmodeltypemadeinBothellCity', '‚óè', '2015\\n‚óèDrawplotstoshowthedistributionbetweencityandelectricvehicletype\\nQuestion10-\\nWriteaprogramtocountthenumberofverbs', 'nouns', 'pronouns', 'andadjectivesinagivenparticularphraseorparagraph', 'andreturntheirrespectivecountasadictionary', 'ExampleOutput-\\nStatistics\\nT o t a l\\nM', '1 2 0\\nE', 'a c h', 'u', 'e', 't', 'o', 'A\\nuniversity', 'the\\nrelationship', 'the\\nSAT\\nscores', 'its \\napplicants', 'their\\ncollege\\nGPA', 'data', '500\\nstudents', 'their\\nSAT\\nscores', 'their\\ncollege\\nGPA', 'a\\n4.0\\nscale', 'the\\ncorrelation', 'SAT\\nscores', 'college\\nGPA', 'this\\ncorrelation\\ncoefficient', 'the\\nrelationship', 'SAT\\nscores', 'college\\nGPA', 'a\\ndataset', 'the\\nheights', 'centimeters', '1000\\nindividuals', 'The\\nmean\\nheight', '170\\ncm', 'a\\nstandard\\ndeviation', '10\\ncm', 'The\\ndataset', 'its\\nskewness', 'this\\ninformation', 'the\\nfollowing\\nquestions', 'a.', 'What\\npercentage', 'individuals', 'the\\ndataset', 'heights', '160\\ncm', '180\\ncm', 'b.', '100\\nindividuals', 'the\\ndataset', 'the\\nprobability', 'their\\naverage\\nheight', 'cm', 'c.', 'the\\ndataset', 'a\\nnormal\\ndistribution', 'a\\nheight', '185\\ncm', 'd.', '5%', 'the\\ndataset', 'heights', 'a\\ncertain\\nvalue', 'this\\nthreshold', 'e.', 'the\\ncoefficient', 'variation', 'CV', 'the\\ndataset', 'f.', 'the\\nskewness', 'the\\ndataset', 'the\\nresult', 'the\\n‚ÄòBlood\\nPressure', '‚ÄòBlood\\nPressure\\nAfter‚Äô\\ncolumns', 'the \\ndata', 'the\\nfollowing\\nhttps://drive.google.com/file/d/1mCjtYHiX--mMUjicuaP2gH3k-SnFxt8Y/view?usp=share', '_', 'the\\ndispersion', 'the\\nresults', 'a\\ngraph', 'c.', 'the\\nMean\\nabsolute\\ndeviation', 'Standard\\ndeviation', 'the\\nresults', 'd.', 'the\\ncorrelation', 'coefficient', 'the\\nsignificance', '1%', 'level', 'significance', 'A\\ngroup', '20\\nfriends', 'a\\ngame', 'a\\nnumber', 'a\\nslip', 'paper', 'a\\nhat', 'slip', 'paper', 'the\\nprobability', 'the\\nnumber', 'the\\nslip', 'paper', 'a\\nperfect\\nsquare', 'A\\ncertain\\ncity', 'two\\ntaxi\\ncompanies', '80%', 'the\\ntaxis', 'Company\\nB', '20%', 'the\\ntaxis', \"Company\\nA's\\ntaxis\", 'a\\n95%\\nsuccess\\nrate', 'passengers', 'time', \"Company\\nB's\\ntaxis\", 'a\\n90%\\nsuccess\\nrate', 'a\\nrandomly', 'the\\nprobability', 'A\\npharmaceutical\\ncompany', 'a\\ndrug', 'blood\\npressure', 'a\\nclinical\\ntrial', '100\\npatients', 'record', 'their\\nblood\\npressure', 'the\\ndrug', 'The\\ncompany', 'the\\nchange', 'blood\\npressure', 'a\\nnormal\\ndistribution', 'The\\nequations', 'two\\nlines', 'regression', 'a\\ncorrelation\\nanalysis', 'variables', 'X', 'Y', '2\\nùëå', 'the\\na.\\nVariance', 'Y\\nb.\\nCoefficient', 'determination', 'C', 'estimate', 'X', 'Y', 'Y', 'X.\\nQ', 'The\\nanxiety\\nlevels', '10\\nparticipants', 'a\\nnew\\ntherapy', 'The\\nscores', 'the\\nWilcoxon\\nsigned-rank\\ntest', 'the\\ntherapy', 'a\\nsignificant\\neffect', 'anxiety\\nlevels', 'The\\ndata', 'Participant', 'therapy', 'therapy', 'the\\nscore', 'students', 'multiple\\nexams', 'the\\nhypothesis', 'the\\nmean\\nscores', 'the\\nstudents', 'the \\nstudent', 'the\\nhighest\\nscore', 'A\\nfactory', 'light\\nbulbs', 'the\\nprobability', 'a\\nbulb', 'The\\nfactory', 'a\\nlarge\\nbatch', '500\\nlight\\nbulbs', 'a.', 'the\\nprobability', 'exactly\\n20\\nbulbs', 'b.', 'the\\nprobability', 'at\\nleast\\n10\\nbulbs', 'c.', 'the\\nprobability', 'max', '15\\nbulbs', 'd.', 'average', 'many\\ndefective\\nbulbs', 'a\\nbatch', 'the\\ndata', 'a\\nfeature', 'different\\nclasses', 'https://drive.google.com/file/d/1mCjtYHiX--mMUjicuaP2gH3k-SnFxt8Y/view?usp', 'share', 'the\\ndistribution', 'the\\nclasses', 'b.\\nCheck', 'the\\nequality', 'c.', 'LDA', 'this\\ndata', 'classification', 'the\\nequality', 'all\\nthe\\nclasses', 'A\\npharmaceutical\\ncompany', 'a\\nnew\\ndrug', 'its \\neffectiveness', 'a\\nstandard\\ndrug', 'a\\nparticular\\ncondition', 'a\\nstudy', 'two\\ngroups', 'the\\nnew\\ndrug', 'Group\\nB', 'the\\nstandard\\ndrug', 'the\\nimprovement', 'a\\nspecific\\nsymptom', 'both\\ngroups', 'a\\n4-week\\ntreatment\\nperiod', 'a.', 'The\\ncompany', 'data', '30\\npatients', 'each\\ngroup', 'the \\nmean\\nimprovement\\nscore', 'the\\nstandard\\ndeviation', 'improvement', 'each\\ngroup', 'The\\nmean\\nimprovement\\nscore', 'a\\nstandard\\ndeviation', 'the\\nmean\\nimprovement\\nscore', 'Group\\nB', 'a\\nstandard\\ndeviation', 'a\\nt-test', 'a\\nsignificant\\ndifference', 'the\\nmean\\nimprovement\\nscores', 'the\\ntwo\\ngroups', 'a\\nsignificance\\nlevel', 'the\\nt-test\\nresults', 'state', 'the\\nnull\\nhypothesis', 'a\\nconclusion', 'the\\ncontext', 'the\\nstudy', '2 1 0\\nE', 'e', 't', 'o', 'a r k s\\nINTERMEDIA TEQUESTIONS', 'Q-1.ImagineyouhaveadatasetwhereyouhavedifferentInstagramfeatureslikeusername', 'Q-2.ImagineyouhaveadatasetwhereyouhavedifferentfeatureslikeAge', 'Gender', 'Height', 'Weight', 'BMI', 'andBloodPressureandyouhavetoclassifythepeopleintodifferentclasseslikeNormal', 'Overweight', 'Obesity', 'Underweight', 'andExtremeObesitybyusingany4differentclassificationalgorithms', 'Nowyouhavetobuildamodelwhichcanclassifypeopleintodifferentclasses', 'Q-3.Imagineyouhaveadatasetwhereyouhavedifferentcategoriesofdata', 'Nowyouneedtofindthemostsimilardatatothegivendatabyusingany4differentsimilarityalgorithms', 'Nowyouhavetobuildamodelwhichcanfindthemostsimilardatatothegivendata', 'Q-4.ImagineyouworkingasasalemanagernowyouneedtopredicttheRevenueandwhetherthatparticularrevenueisontheweekendornotandfindthe\\nInformational_DurationusingtheEnsemblelearningalgorithmDatasetThisistheDatasetYoucanusethisdatasetforthisquestion', 'Q-5.Uberisataxiserviceproviderasweknow', 'weneedtopredictthehighbookingareausinganUnsuper visedalgorithmandpriceforthelocationusingasupervisedalgorithmandusesomemapfunctiontodisplaythedataDatasetThisistheDatasetYoucanusethisdatasetforthisquestion', 'Q-6.ImagineyouhaveadatasetwhereyouhavepredictedloanEligibilityusingany4differentclassificationalgorithms', 'NowyouhavetobuildamodelwhichcanpredictloanEligibilityandyouneedtofindtheaccuracyofthemodelandbuilt-indockerandusesomelibrarytodisplaythatinfrontendDatasetThisistheDatasetYoucanusethisdatasetforthisquestion', 'Q-7.ImagineyouhaveadatasetwhereyouneedtopredicttheGenresofMusic', 'anUnsuper visedalgorithmandyouneedtofindtheaccuracyofthemodel', 'built-indocker', 'andusesomelibrarytodisplaythatinfrontendDatasetThisistheDatasetYoucanusethisdatasetforthisquestion', 'Q-8.Quoraquestionpairsimilarity', 'andusingasupervisedAlgorithmyouneedtofindthesimilaritybetweenthequestions', 'Q-9.AcybersecurityagentwantstochecktheMicrosoftMalwaresoneedhecametoyouasaMachinelearningEngineeringwithData', 'AdvanceQUESTIONS', 'Q-1.ASocialMediaInfluencercollecteddataonFacebookfriendrequestsandusedasupervisedalgorithmtopredictwhetherauserwouldacceptafriendrequestornot', 'Note', 'UseonlyDaskandUseMLflow\\nQ-2.Achemisthadtwochemicalflaskslabeled0and1whichconsistoftwodifferentchemicals', 'youprovidedtheresultsderivedbythechemicalsandyourtaskistocreateamodelthatwilllabelchemical0or1givenitsthreefeaturesandbuilt-indockerandusesomelibrarytodisplaythatinfrontend', 'Note', 'Q-3.Acompanywantstopredictthesalesofitsproductbasedonthemoneyspentondifferentplatformsformarketing.Theywantyoutofigureouthowtheycanspendmoneyonmarketinginthefutureinsuchawaythattheycanincreasetheirprofitasmuchaspossiblebuilt-indockerandusesomelibrarytodisplaythatinfrontendDatasetThisistheDatasetYoucanusethisdatasetforthisquestion', 'Note', 'a l\\nM', '1 0 0\\nE', 'e', 't', 'o', 'a r k s\\nQuestion1-Implemen', 'Question2-Implemen t5differentCNNarchitectureswithacomparisontableforCIFAR10datasetusingthePyTorchlibraryNote-1.Themodelparametersforeacharchitectureshouldnotbemorethan10000parameters', 'Question3-TrainaPureCNNwithlessthan10000trainableparametersusingtheMNISTDatasethavingminimumvalidationaccuracyof99.40%Note-1.Codecommentsshouldbegivenforpropercodeunderstanding.2.Implemen', 'wrespectiv', 'Trytomixthebestofbothservices\\nQuestion5-\\nInQuestion4,youhavedesignedthearchitectureforanobjectdetectionusecaseleveragingAWSCloud', 'similarly ,hereyouwillbedesigningforDocumen tClassific ationusecaseleveragingAzureCloudservices', 'erVision\\nT o', 'a l\\nM', '2 0 0\\nE', 'e', 't', 'o', 'a r k s\\nQuestion1-Trainadeeplearningmodelwhichwouldclassifythevegetablesbasedontheimagesprovided', 'Thedatasetcanbeaccessedfromthegivenlink', 'Link-https://www.kaggle.com/datasets/misrakahmed/v', 'egetable-imag e-dataset\\nNote-1.UsePyTorchastheframeworkfortrainingmodel2.UseDistributedParallelTrainingtechniquetooptimizetrainingtime.3.Achieveanaccuracyofatleast85%onthevalidationdataset.4.Usealbumen', 'tationslibraryforimagetransformation5.UseTensorBoar', 'Question2', 'YouneedtoconvertthetrainedmodeltoONNXformatandachievefasterinferenceNote-1.Thereisnosetinferencetime,buttrytoachieveaslowaninferencetimeaspossible2.Createawebapptointeractwiththemodel,wheretheusercanuploadtheimageandgetpredictions3.Trytoreducethemodelsizeconsiderablysothatinferencetimecanbefaster4.UsemodularPythonscriptstotrainandinferthemodel5.OnlyJupyternotebookswillnotbeallowed6.WritecodecommentswheneverneededforunderstandingQuestion3-Scraptheimagesfrompopulare-commercewebsitesforvariousproductimagessoldonthosewebsites', 'Note-1.YoucanuseanyframeworkofyourchoicelikeTensorFlo', 'worPyTorch2.Youhavetonotuseanypre', 'trainedmodel', 'Yourgoalistodetectdifferentproductsbasedonthegivenclassesbasedontheuserinput', 'YJwO_PVVfAbyfjaRHXt7qoiBBHY', 't', '=', 'Note-1.YouhavetousePyTorchimplemen tationofYOLOV72.Thedatasetconsistsof102classeswithtrain', 'validation', 'andtestimagesalreadyintherespectiv efolders.3.Labelingisalreadydone', 'givenwiththedataset', 'soneedforannotation4.Sincethedatasetissmall', 'Question5', 'FromQuestion4,youwouldhaveacustom', 'trainedYOLOmodel', 'YourgoalistoneedtoconvertthemodeltoONNXformatandreducetheinferencetime.\\nNote-1.Reducetheinferencetimetoasmuchaspossible2.TrytoreducethemodelsizebyusingtechniqueslikeQuantization', 'Question6-Youhavetotrainacustomsegmen tationmodelbasedonDetectron2framework', 'Yourgoalistosegmen', 'es-2019-ins tance-segmen tation/data', 'Question7-FromQuestion6,youwouldhavecustomtrainedsegmen tationmodel', 'Yourgoalistoreducethemodelinferencetime', 'Question8', 'YouhavetotrainacustomobjectdetectionmodelbasedonDETR(DetectionTransformer', 'Link-https://www.kaggle.com/datasets/andrewmvd/helme t-detection', 'Question9-FromQuestion8,youwouldhaveacustomobjectdetectionmodelNote-1.Trytoreducethemodelsizeusingquantization2.Createawebappwheretheuserscaninteractwithyourmodel3.WritemodularPythonscriptformodelinference4.OnlyJupyterNotebooksarenotallowed5.Writecodecommentswhereverneededforcodeunderstanding\\nQuestion10-Fromallthequestionsfrom1to9,takeanyimageclassificationmodel', 'Azure', 'andGCP2.Avideodemooftheapplicationworkinginthecloudshouldbegoodenough3.Containerizationofall3applicationsisimportantandshouldbepushedtoDockerHubComputerVisionAssessmentiNeuron54.CI', 'NaturalLanguag eProcessing\\nT o', 'a l\\nM a r k s :\\n2 0 0\\nE', 'a c h', 'u', 'e', 't', 'o', 'a r k s\\nQ-1.TakeanyYouTubevideoslinkandyourtaskistoextractthecommentsfromthatvideosandstoreitinacsvfileandthenyouneeddefinewhatismostdemandingtopicinthatvideoscommentsection', 'Q-2.Takeanypdfandyourtaskistoextractthetextfromthatpdfandstoreitinacsvfileandthenyouneedtofindthemostrepeatedwordinthatpdf', 'Q-3.fromquestion2,AsyougottheCSVandnowyouneedperformkeywordextractionfromthatcsvfileanddotheTopicmodeling\\nQ-4.TakeanytextfileandnowyourtaskistoTextSummariz ationwithoutusinghuggingtransformerlibrary\\nQ-5.Nowyouneedbuildyourownlanguag edetectionwiththefastTextmodelbyFacebookand\\nQ-6.GenerateresearchpaperstitlesusingBertmodelandcontainerizetheapplicationandpushtopublicdockerhub\\nQ-7.Nowyouneedtobuildyourownchatbotusingtheseq2seqmodelofAmazonwebsitebyscrapethewebsiteandcontainerizetheapplicationandpushtopublicdockerhub\\nQ-8.TakeaanyowndatasetandbuildaknowledgebotusingLlamamodel', 'Q-9.Usingwisheryouneedtranscribeanyaudiofileandthenyouneedtoconvertthataudiofileintotextfileandnowconvertthattextfileintoaudiofileofdifferentlanguag e.\\nQ-10.BuildawholeEnd-EndapianddeployitonHeroku/railwayssothetaskisthatyouneedbuildaAuto-CorrectionoftextusingNLPNote:onlyJupyternotebookisnotallowedfrom5thquestion‚Üê\\nS', 'i o n\\nP r o', 'Two\\nTypes', 'Questions\\nTheory\\nbased\\nQuestion', 'code', 'an\\nGoogle\\ndoc', 'answers', 'all\\nthe\\nquestions', 'a\\nquestion', 'code', 'your\\ncode', 'the\\nlink', 'repo', 'docs', 'E g', 'A n s w e r', 'y t', 'H', 'u', 'r e', 'N o t e', 'a r e', 'l', 'n', 'i t e', 'e', 'l', 'a n', 'a l', 'o', 'd', 'o', 'g\\nE D A\\nu', 'e', 'y', 'f', 'l', 'a\\ntheory-based\\nquestion', 'the\\nanswer', 'the\\nsame\\ngoogle\\ndocs', 'that\\nfinal\\nlink', 'google\\ndoc\\nlink', 'all\\nthe\\nanswers']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dominant_topic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l04f-wwVUE0r",
        "outputId": "eb6eea6f-3119-470f-d1b0-fe121eb941c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 0.9993489)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model.show_topic(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHqRhqDBXQ0w",
        "outputId": "7aaba2fa-f340-4167-dc76-a69b342587a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 0.06426435),\n",
              " ('of', 0.033328097),\n",
              " ('and', 0.028053306),\n",
              " ('question', 0.01976018),\n",
              " ('is', 0.016729277),\n",
              " ('note', 0.01599743),\n",
              " ('to', 0.014480916),\n",
              " ('in', 0.011458014),\n",
              " ('on', 0.009202348),\n",
              " ('that', 0.008445741)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pLI6kPO7YjXI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}